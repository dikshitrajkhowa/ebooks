1. Install java to /opt/local/java folder
	sudo mkdir -p /opt/local/java
	cd /opt/local/java
    sudo cp /home/thejaswi/install/jdk-7u40-linux-i586.tar.gz .
    sudo tar xzf jdk-7u40-linux-i586.tar.gz 

2. Adding a dedicated Hadoop system user. (Provide password as hduser)
	sudo addgroup hadoop
	sudo adduser --ingroup hadoop hduser

3. Install ssh and rsync (First timers run sudo apt-get update)
	sudo apt-get install ssh 
	sudo apt-get install rsync

4. Configuring SSH
	su - hduser
	ssh-keygen -t rsa -P ""
	cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys
	ssh localhost	#This is required to save your local machine’s host key fingerprint to the hduser user’s known_hosts file
	
	exit
	ssh localhost   #Shouldn't ask anything

	SSH Debug Options
	1. ssh -vvv localhost
	2. Check /etc/ssh/sshd_config
		PubkeyAuthentication should be yes
		AllowUsers if it is there should have hduser
		Reload if changed : sudo /etc/init.d/ssh reload

5. Unzip hadoop
	cd /opt/local
	sudo cp /home/thejaswi/install/hadoop-1.2.1.tar.gz .
	sudo tar xzf hadoop-1.2.1.tar.gz
	sudo mv hadoop-1.2.1 hadoop
	sudo chown -R hduser:hadoop hadoop

6. Update $HOME/.bashrc (Run ". $HOME/.bashrc" afterwards)

# Set Hadoop-related environment variables
export HADOOP_HOME=/opt/local/hadoop

# Set JAVA_HOME (we will also configure JAVA_HOME directly for Hadoop later on)
export JAVA_HOME=/opt/local/java/jdk1.7.0_40

# Some convenient aliases and functions for running Hadoop-related commands
unalias fs &> /dev/null
alias fs="hadoop fs"
unalias hls &> /dev/null
alias hls="fs -ls"

# Add Hadoop bin/ directory to PATH
export PATH=$PATH:$HADOOP_HOME/bin

# Add Java bin/ directory to PATH
export PATH=$PATH:$JAVA_HOME/bin

7. Create /app/hadoop/tmp (Which is our hadoop.tmp.dir)
		sudo mkdir -p /app/hadoop/tmp
		sudo chown hduser:hadoop /app/hadoop/tmp
	
8. Configuration (/opt/local/hadoop)
	conf/hadoop-env.sh
		export JAVA_HOME=/opt/local/java/jdk1.7.0_40
		
	conf/core-site.xml	
	<property>
	  <name>hadoop.tmp.dir</name>
	  <value>/app/hadoop/tmp</value>
	  <description>A base for other temporary directories.</description>
	</property>

	<property>
	  <name>fs.default.name</name>
	  <value>hdfs://localhost:54310</value>
	  <description>The name of the default file system.</description>
	</property>

	conf/mapred-site.xml
	<property>
	  <name>mapred.job.tracker</name>
	  <value>localhost:54311</value>
	  <description>The host and port that the MapReduce job tracker runs at.</description>
	</property>

	conf/hdfs-site.xml
	<property>
	  <name>dfs.replication</name>
	  <value>1</value>
	  <description>Default block replication.</description>
	</property>

9. Formatting the HDFS filesystem via the NameNode
	(Do not format a running Hadoop filesystem as you will lose all the data currently in the cluster (in HDFS)!)
	hduser@ubuntu:~$ /opt/local/hadoop/bin/hadoop namenode -format
	or
	hduser@ubuntu:~$ hadoop namenode -format

10. Starting your single-node cluster
	hduser@ubuntu:~$ start-all.sh
	
	# Test whether it is started
	$ jps
	$ 12537 Jps
	$ 12074 JobTracker
	$ 11608 NameNode
	$ 11799 DataNode
	$ 11998 SecondaryNameNode
	$ 12269 TaskTracker

	# You can also use
	sudo netstat -plten | grep java

11. Stopping your single-node cluster
	hduser@ubuntu:~$ stop-all.sh
	
	Hadoop Web Interfaces
    http://localhost:50070/ – web UI of the NameNode daemon
    http://localhost:50030/ – web UI of the JobTracker daemon
    http://localhost:50060/ – web UI of the TaskTracker daemon

12. Running a MapReduce job
	* Start the hadoop cluster
		hduser@ubuntu:~$ start-all.sh
		
	* Copy local example data to HDFS
		hduser@ubuntu:~$ hadoop fs -copyFromLocal /home/thejaswi/gutenberg/ gutenberg
		hduser@ubuntu:~$ fs -ls /user/hduser #fs alias to hadoop fs
	
	* Run the MapReduce job
		hduser@ubuntu:/opt/local/hadoop$ hadoop jar hadoop*examples*.jar wordcount /user/hduser/gutenberg /user/hduser/gutenberg-output
		or
		hduser@ubuntu:/opt/local/hadoop$ hadoop jar hadoop-examples-1.2.1.jar wordcount /user/hduser/gutenberg /user/hduser/gutenberg-output
		
		This command will read all the files in the HDFS directory /user/hduser/gutenberg, process it, and store the result in the HDFS directory /user/hduser/gutenberg-output
		Check if the result is successfully stored in HDFS directory /user/hduser/gutenberg-output
		
		hduser@ubuntu:/opt/local/hadoop$ fs -ls /user/hduser
13. If you want to modify some Hadoop settings on the fly like increasing the number of Reduce tasks, you can use the "-D" option:
	hduser@ubuntu:/opt/local/hadoop$ hadoop jar hadoop*examples*.jar wordcount -D mapred.reduce.tasks=16 /user/hduser/gutenberg /user/hduser/gutenberg-output
	
	An important note about mapred.map.tasks: Hadoop does not honor mapred.map.tasks beyond considering it a hint. But it accepts the user specified mapred.reduce.tasks and doesn’t manipulate that. You cannot force mapred.map.tasks but you can specify mapred.reduce.tasks. 

14. Retrieve the job result from HDFS
	hduser@ubuntu:/opt/local/hadoop$ fs -cat /user/hduser/gutenberg-output/part-r-00000 | head
	
	hduser@ubuntu:/opt/local/hadoop$ mkdir /tmp/gutenberg-output
	hduser@ubuntu:/opt/local/hadoop$ bin/hadoop fs -getmerge /user/hduser/gutenberg-output /tmp/gutenberg-output
	hduser@ubuntu:/opt/local/hadoop$ head /tmp/gutenberg-output/gutenberg-output
	
	The command fs -getmerge will simply concatenate any files it finds in the directory you specify. This means that the merged file might (and most likely will) not be sorted.
	